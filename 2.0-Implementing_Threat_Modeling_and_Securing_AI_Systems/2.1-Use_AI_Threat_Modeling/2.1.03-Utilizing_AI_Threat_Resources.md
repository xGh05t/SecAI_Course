
# 2.1.3 Utilizing AI Threat Resources

As AI usage increases across different use cases within an organization, it increases the organization's attack surface and attack vectors. As a cybersecurity professional, it is essential to understand the different threat scenarios and risks for AI systems. Security teams must analyze the risks not only related to the technical aspects but also behavioral, data-driven, and ethical. So, there is a need for referencing AI threats over time and the defense strategies used for mitigating them.

To address the challenges of identifying threats, several AI threat repositories have been developed. These repositories consist of known AI vulnerabilities, real-time incidents, and research findings. These are designed to help security teams identify and map threats in the model development lifecycle, such as the collection of data, training models, and user interaction. Some of the widely recognized and used AI threat resources are the ***Massachusetts Institute of Technology (MIT) AI Risk Repository***, the ***Common Vulnerabilities and Exposures (CVE)*** system, the ***AI Vulnerability Database (AVID)***, the AI Incident Database (AIID), and arXiv.

## MIT AI Risk Repository

***MIT AI Risk Repository*** is one of the major sources of AI threats helping security personnel. The repository contains over 1,600 AI risks sourced from 65 existing frameworks, seven domains, and 24 subdomains. It includes the classification of AI risks into taxonomies on the basis of how, when, why, and in what domain the risks occurred. This repository is updated on a regular basis and acts as a live database.

MIT AI risk repository contains three parts:

- AI Risk database—This is the centralized database with over 2,244 records, each record defining an AI risk. The database includes the title of the paper and provides keywords for quick reference. The evidence for each risk is collected and grouped into risk categories. Each risk is explained in detail and supported by evidence from the sources. In addition, the risks are classified into both causal and domain taxonomies.

- Causal Taxonomy of AI Risks—This taxonomy classifies risks based on how they are originated and whether a risk is caused by a decision or action made by AI, human user/developer, or other external factors. It distinguishes whether the risk is caused intentionally, like the expected outcomes of a goal, unintentionally, or even classified as "other", which can't be determined. It also categorizes the risk based on the timing of the risk identified, like before the deployment of the model or if the risk occurs after the training and deployment. This helps security teams trace back the steps to exactly when the risk was identified during the model development lifecycle.

- Domain Taxonomy of AI Risks—The domain taxonomy classifies risks into seven AI risk domains and 23 subdomains, such as (1) Discrimination & Toxicity, (2) Privacy & Security, (3) Misinformation, (4) Malicious Actors & Misuse, (5) Human-Computer Interaction, (6) Socioeconomic & Environmental, and (7) AI System Safety, Failures, & Limitations. These domains help security teams search for keywords and identify risks within the AI system across these domains.

For instance, let's consider the domain (1) Discrimination & Toxicity. It contains three subdomains of AI risks, such as (1.1) Unfair discrimination and misrepresentation, (1.2) Exposure to toxic content, and (1.3) Unequal performance across groups. The repository highlights policy and governance gaps, such as a lack of observability, transparency, and unclear responsibilities regarding the usage of AI. These complications can have legal and compliance implications, as they impact users directly or affect decision-making.

Security professionals can use the repository's structure to create threat matrices, build incident playbooks, or perform gap analysis against different frameworks.

## CVE AI Workgroup

CVE AI workgroup is part of the common vulnerability enumeration (CVE) program, which is a committee of members from within the board of CVE, corporate members who perform vulnerability management, members of the AI community, and their associations.

This working group develops policies and guidelines for identifying and reporting AI-based vulnerabilities. This group would analyze and identify AI threats, assigning them a CVE ID and making them part of the CVE database.

The group is working on creating clear criteria, taxonomies, and best practices for documenting AI risks so they can be shared across the AI and cybersecurity communities. This includes integrating AI-specific entries into the broader CVE framework. The taxonomies help categorize vulnerabilities based on causes (e.g., data poisoning, algorithmic errors) and domains (e.g., computer vision, natural language processing). This allows researchers, developers, and security analysts to understand the nature of each risk and its impacts.

## AI Vulnerability Database

***AI Vulnerability Database (AVID)*** is an open-source knowledge base that collects data about failure modes for AI models, datasets, and systems. This database would provide a structured approach for security teams to identify, assess, and mitigate AI-specific vulnerabilities. Security teams or development teams, before deploying any AI component, can search AVID for known vulnerabilities that may affect the model, dataset, or application programming interface (API) they plan to use.

## AI Incident Database

This is a collection of AI incidents that happened in real-time. The collection includes vast information from the Internet on everyday threats caused by AI systems, like deepfakes, biases, and misuse of AI.

This database helps security teams and researchers to study these incidents; threat hunters can identify common attack patterns and vulnerabilities that may exist in their own systems. It helps security teams understand the emerging risks, analyze how attackers target AI models, and assess the AI systems for weaknesses in data handling, training processes, or model deployment.

## arXiv

***arXiv*** is an open-source platform where researchers share scientific papers. It covers the research from different fields of study, including artificial intelligence (AI). Researchers around the world upload their latest findings to arXiv to share knowledge and get feedback from the community.

AI threat resource is a collection of threats and real-time incidents that help identify and understand risks in artificial intelligence systems. They are important because AI is used in many critical areas and would have high business impact, and legal and compliance consequences. These resources provide insights into how threats have been identified in the past, what weaknesses may exist, and how to prevent or remediate the issues identified. By using this information, developers and security teams can improve a safe, reliable, and responsible AI system.

## Common Weakness Enumeration

The ***Common Weakness Enumeration (CWE)*** is a widely used framework for identifying and categorizing software vulnerabilities. With respect to AI and model security, CWE can be adapted to map real-world threats to specific types of system weaknesses. For example, CWE-77 (Command Injection) is relevant for prompt injection attacks where user input manipulates model instructions. CWE-200 (Exposure of Sensitive Information) aligns with risks like system prompt leakage or model inversion. Using CWE helps standardize the threat landscape for AI systems, making it easier to classify, report, and mitigate vulnerabilities. Security teams can use CWE mapping during risk assessments, red teaming, and secure development to track exposure consistently. This also supports integration with other frameworks like MITRE ATT&CK or CVE databases.