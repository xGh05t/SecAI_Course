
# 2.1.1 Introduction to AI Threat Modeling

## What is AI Threat Modeling?

***AI threat modeling*** is a process of identifying potential threats and analyzing the risks associated with AI systems. Traditional threat modeling will not address the unique vulnerabilities introduced by AI, such as data poisoning, prompt injection, or model extraction attacks. This process helps security teams to analyze the applications and identify both traditional and AI-related threats.

Unlike the threat modeling of a conventional application, this process focuses on several unique aspects and a step-by-step approach within AI application architecture to identify threats.

AI systems need more computational power when compared to traditional systems, and it is therefore essential to understand the infrastructure where the application is hosted, as well as its scalability. Focusing on sources of data and the type of data used for training the models is required, as data collection can introduce threats such as biases, misinformation, or harmful content being shared with users. It is important to analyze how access is provided to AI and its underlying back-end systems for performing various actions, such as retrieving, modifying, deleting, etc. Misconfiguration of access controls would help adversaries gain access to data and perform malicious activities. Evaluating the different types of AI models used in the applications, such as third-party models or in-house developed models, is required to analyze the supply chain risks

The AI applications work by collecting data from users and responding back with information, so the data shared with AI systems must be stored and processed securely. Evaluating the guardrails implemented around AI usage will help prevent sensitive information leakages and help set up boundaries on how AI must respond back to users based on the organizational standards. AI systems are highly unpredictable, and the user interactions with AI have to be logged and monitored to observe changes in model behavior. Assessing AI logging and monitoring controls should include data classification, storage, retention, and relevant regulatory requirements be factored for effective security compliance and monitoring.

AI systems can introduce new threat surfaces and attack vectors, unlike the traditional applications. Hence, a threat modeling specific to AI systems is needed.

## Why Do We Need AI Threat Modeling?

AI has evolved from simple automation to generating synthetic information, decision-making, real-time assistance, learning from conversations, connecting complex systems, etc. The usage of AI is not just confined to one industry or use case; it has been extended to support humans in multiple activities across various sectors. Some examples include using AI as knowledge bots for assisting users as coding assistants for developers, interactive assistants to help increase the customer experience, and for fraud detection.

To perform the tasks, AI would require access to data, internal systems, applications, etc. Improper configuration of access controls and guardrails could provide sensitive information back to unauthorized users, compromising the organization's data and affecting confidentiality. Attackers can ***jailbreak*** the AI models and perform data poisoning, which can lead to data integrity issues and spreading ***misinformation***. Lack of rate limitation on the usage of AI leads to unlimited utilization of services, causing exhaustion of back-end resources, financial impact, and availability issues. Performing AI threat modeling in the early stages of Secure ***Software Development Life Cycle (SDLC)*** would help identify and prevent the risks associated with using AI in applications.