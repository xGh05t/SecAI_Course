
# 1.2.9 Live Lab: Perform Prompt Engineering and Bias Detection

## Scenario
In this lab you will explore how the wording of prompts, the choice of AI models, and the presence of contextual details all affect the quality of AI-generated responses. You will also learn how to detect and reduce bias in those responses.

> [!NOTE]
> **The Mission**:
> - Apply stepwise refinement to improve prompt clarity and actionability.
> - Adapt prompts for different audiences.
> - Use few-shot examples to enforce structured response formats.
> - Identify bias in AI-generated recommendations and redesign prompts to mitigate it.

## Exam Objectives
This activity is designed to test your understanding of and ability to apply content examples in the following CompTIA SecAI+ objectives:

- 1.1 Compare and contrast various AI types and techniques within the context of cybersecurity.

---

## Prompt Refinement Basics
The way a prompt is written directly affects the quality of an AI response. In this task, you'll practice refining prompts step-by-step and measure how each version improves clarity, specificity, tool awareness, and actionability.

> [!NOTE]
> **Mistral** is a family of lightweight open-source large language models designed for efficiency and speed. They generally produce concise, structured, and technically clear outputs, which makes them a strong choice for tasks like:
> - Summarizing or condensing large amounts of text
> - Providing step-by-step technical guidance
> - Security analysis where accuracy and brevity matter
> - Running locally with limited compute resources

Adjust **Chat Controls - Advanced Parameters** settings to optimize performance. Adjust the settings below:

| Parameter | Value | Why |
| :---: | :---: | :---: |
| Stream Chat Response | On | This returns tokens as they are generated, so you will see answers immediately, rather than waiting on the entire response. |
| num_thread | 8 | Allows the model to use more CPU cores in parallel |
| num_batch | 1024 | Higher batch sizes allow the model to process more tokens at once. |

**AI Prompt:**
```
As a SOC analyst, explain the risks of storing sensitive data in the cloud in the context of compliance with HIPAA and GDPR.
```

> [!NOTE]
> When running the same HIPAA compliance prompt across models:
> - **Phi-3** produces a much longer, more expansive answer. Phi-3's style can be useful if you want depth, narrative, or broader context, but it also means the response may wander beyond the immediate scope of the question.
> - **Mistral's** response is shorter and more focused on the specific risks. Mistral, on the other hand, tends to be more concise and directly relevant to the prompt, which can save time.

This illustrates how switching models is part of **prompt engineering** - choosing the model that naturally aligns with the style of response you need.

---

## Zero-Shot vs Few-Shot Prompting
AI models will usually give some kind of answer to an open-ended question, but the format and consistency can vary. By providing structured examples, you can guide the model toward more predictable and reusable outputs. This technique is called few-shot prompting. In this task, you'll compare how the model responds with no examples (zero-shot) versus with two short examples (few-shot).

**AI Prompt:**
```
What steps should a SOC analyst take to investigate a workstation that is sending a large amount of data to an external IP address?
```

**AI Prompt2:**
```
I need structured investigation steps for security incidents. Here are examples:

Example 1 - Malware Detection:
Input: Malware detected on workstation
Steps:
1. ISOLATE: Disconnect workstation from network
2. PRESERVE: Create forensic image
3. ANALYZE: Run behavioral analysis
4. DOCUMENT: Record findings and timestamps

Example 2 - Phishing Email: 
Input: User reported suspicious email
Steps:
1. QUARANTINE: Isolate email from mailboxes
2. INVESTIGATE: Check sender reputation
3. ANALYZE: Examine attachments safely
4. COMMUNICATE: Alert users if needed

Current Incident - Suspicious Outbound Traffic:
Input: Workstation DESK-001 sending 2.3 GB to external IP 185.234.72.156
Steps: [Please provide structured format]
```

**AI Prompt3:**
```
Using the same current incident as above, produce exactly four lines with these labels and no extra text:

1. ISOLATE: [8 words or less]
2. PRESERVE: [8 words or less]
3. ANALYZE: [8 words or less]
4. DOCUMENT: [8 words or less]
```

Few-shot prompting can do more than just improve structure-it can also lock the model into a very specific output style. By setting explicit constraints, you guide the model to produce predictable, uniform results. If the model drifts, you can re-prompt to reinforce the constraints, which is a core part of prompt engineering.

---

## Bias Detection in AI

### Detecting Positivity Bias
AI systems are tuned to be helpful and optimistic. While that makes answers easier to read, it can sometimes hide limitations or risks. In this task, you'll compare a straightforward "setup" answer with a deeper, logic-driven critique that uncovers gaps, drawbacks, or missing context.

**AI Prompt1:**
```
How do I set up cloud storage for sensitive company data?
```

The answer will probably sound straightforward, but leaves out caveats.

**AI Prompt2:**
```
Break down the user's needs in detail. List as many distinct requirements and concerns as possible without grouping them into broad categories.
Include both benefits and drawbacks realistically.
If any important context is missing, ask clarifying questions before giving a final recommendation.
```

> [!NOTE]
> The model's first instinct is to provide a smooth, optimistic solution that may skip over risks.
> - By reframing the prompt to explicitly demand concerns, drawbacks, and clarifying questions, you force the AI into a more balanced, realistic mode.

---

### Detecting Confirmation Bias
Confirmation bias is the tendency of AI models to agree with the framing of a question, even when the premise is overstated or flawed. Instead of challenging the assumption, the model often softens it slightly and then justifies it.

> [!NOTE]
> By comparing answers to leading prompts against neutrally phrased prompts, notice how the model can drift into supporting assumptions instead of critically evaluating them.
> - Recognizing this helps you design prompts that avoid reinforcing flawed or one-sided reasoning.

**AI Prompt1:**
```
Why is the Finance department always the highest security risk in a company?
```

The model didn't call out the flawed assumption ("always") directly; instead it subtly reframed the wording and then justified the idea.

**AI Prompt2:**
```
Some people say Finance is always the highest security risk in a company.
Provide arguments for and against this claim, and explain when it may or may not be true.
```

> [!NOTE]
> The model should produce a balanced, more critical answer. By carefully rephrasing prompts, you can encourage the model to give a more balanced evaluation instead of echoing your assumptions.

---

### Prejudicial Bias
Prejudicial bias occurs when AI systems treat people differently based on protected attributes such as:
- Age
- Gender
- Race
- Religion

Models are trained to avoid reinforcing stereotypes, and when prompted with these kinds of questions, usually give a neutral, balanced answer.

**AI Prompt1:**
```
Which gender makes better cybersecurity professionals?
```

This is a clearly prejudicial question. The model should refuse to compare by gender and redirect to skills/experience.
