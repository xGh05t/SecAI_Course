
# 1.3.2 Data Security Considerations for AI

Artificial intelligence thrives on data. Whether it's a script that flags suspicious activity, a language-model system that condenses an incident report, or a search tool that retrieves the right policy paragraph from a database, each tool is only as trustworthy and secure as the data it consumes. For example, AI amplifies defensive capabilities, but the data that fuels it must also be protected with the same rigor as any other critical asset. Neglecting AI data protections creates an avenue for compromise, ranging from model poisoning to intellectual property theft.

## Data Pipeline Hardening

Security teams play a vital role in managing the flow of data related to logs, packet captures, and threat intelligence feeds. One of the key challenges they face is integrating artificial intelligence (AI) into these processes, which can introduce new risks.

When data is collected, it's essential to verify its integrity. For example, checking that the data is complete, accurate, and can be traced back to its original source. By ensuring that incoming data meets these criteria, organizations can help safeguard against errors or manipulation that could jeopardize the AI training process. Once the data has been verified, it must be cleaned and standardized to prevent bias from influencing the AI model. If the data is not uniform, it could skew the AI's outputs and predictions. Additionally, from initial collection to data transfer between systems, encryption must be utilized to protect sensitive information. Once the data is securely stored, it must remain encrypted, with the decryption keys kept in a separate, heavily secured location.

## Structured, Semi-Structured, and Unstructured Security Data

***Structured data*** plays a key role in traditional security analytics. This type of data includes firewall logs, NetFlow records, and indicators of compromise (IOC) lists, all of which are organized in specific formats that make them easier to analyze and use. However, there's a significant risk associated with exposing this type of information. For instance, if a dataset containing malicious hashes is shared without proper sanitization, it might unintentionally reveal sensitive operational techniques and strategies used by security teams. This could give attackers valuable insights into how organizations defend themselves, potentially allowing them to adjust their tactics accordingly. Moreover, time-stamped log data can provide critical information about a company's operational patterns. For example, analyzing log files might uncover when employees are usually at work, revealing corporate operating hours and shift schedules. Armed with this knowledge, attackers could plan their strategies around these times, targeting the organization when it is most vulnerable.

***Semi-structured*** data sources are types of data formats that combine elements of both structured and unstructured data. Common examples include JSON, email headers, and YAML. These formats typically organize information using a key-value structure, where each key corresponds to a specific piece of data (the value). These data sources can sometimes unintentionally include sensitive information. For instance, authentication tokens or passwords may be embedded in JSON files. If this sensitive data is not properly managed, it could pose security risks, such as unauthorized access to systems or applications. To mitigate these risks, strategies to identify and remove sensitive information from these data formats must be used. This often involves using automated scanning tools that can analyze the text and locate sensitive information, such as passwords or access keys, and remove or anonymize it before it is used.

***Unstructured data*** includes things like packet payloads from network traffic, chat transcripts from analyst discussions, and images taken from security cameras. These data types contain valuable context that can be leveraged for various purposes, but they also pose risks. For example, packet payloads may contain sensitive information that can be analyzed by malicious actors, and chat transcripts can unintentionally reveal sensitive information such as employee names, system information, defensive techniques, and operational capabilities. Similarly, if a language model trained on help tickets is not carefully managed, it might expose this sensitive information when it generates responses. Image classifiers (tools that analyze and categorize images) can also pose risks. When analyzing photos of buildings or secure areas, these systems might inadvertently disclose important details about a facility's layout or security capabilities, which could be exploited.

## Dataset and Model Watermarking

Watermarking plays a crucial role in the broader context of securing AI systems and the data they rely on. As organizations increasingly depend on AI algorithms to analyze and interpret data, the potential for misuse of proprietary information grows significantly. With these risks in mind, watermarking provides data protection and also serves as a strategic tool for establishing ownership and accountability.

The importance of watermarking is underscored by growing concerns over data integrity and authenticity in the era of AI. Watermarking provides organizations with a mechanism by which the source of any information can be validated as necessary. By embedding invisible identifiers within AI-generated content (such as text, images, or audio), developers incorporate a verifiable trail that establishes the origin of the data. These protections are crucial for machine learning processes, as training datasets often contain sensitive or proprietary information.

When AI systems ingest compromised or tampered data, the results often yield poor outputs that lead to inaccurate outputs (hallucinations) and unintended consequences. Watermarks enable organizations to track data throughout its lifecycle, ensuring its integrity and that AI system outputs are reliable and trustworthy. In a legal context, watermarking protects organizations against infringement. By demonstrating a clear link between the data and its rightful owner, watermarking supports an organization's legal standing should disputes arise regarding content usage. In situations involving machine-generated outputs, the line between creativity and copyright infringement can be blurry, and watermarking can go a long way to support ownership claims. Additionally, model watermarking adds another layer of security, specifically targeting the algorithms that drive AI systems. By embedding specific test prompts that produce repeatable and uniquely recognizable outputs, developers can safeguard their models from duplication or reverse engineering by competitors or other threat actors. This type of protection allows companies to invest in AI research and development with greater confidence, knowing that their work is safeguarded.

## Retrieval-Augmented Generation

Large models fine-tuned on internal incident reports offer powerful summarization capabilities, but rebuilding them after each document update is impractical. ***Retrieval‑augmented generation (RAG)*** addresses this problem by pushing fresh knowledge into ***vector storage*** (a lookup table that turns pieces of text into numeric fingerprints called ***embeddings***) while keeping the foundation model (the large, general‑purpose AI system already trained on vast, diverse data) unchanged. However, a RAG system also represents an attack target that contains large volumes of potentially sensitive information. Some examples of protections designed to protect RAG data include encryption of the vector index (the database that stores those numeric fingerprints), tenant isolation to prevent one department from accidentally or deliberately viewing another's proprietary information, and sanitizing all questions and documents entering the system to block ***prompt‑injection attacks*** before they reach the model. This is not a comprehensive list of the controls available to protect RAG storage, but instead identifies some of the unique considerations required when protecting AI platforms.

AI offers compelling advantages to cyber operations capabilities. Yet, if attackers can poison the data, the very intelligence built to help defend the enterprise can veer off course. AI's promise in cybersecurity hinges on disciplined data stewardship. By mastering modern data processing techniques, understanding the distinct vulnerabilities of structured, semi‑structured, and unstructured data inputs, and adopting watermarking and secure RAG architectures, cyber practitioners mitigate their associated risks.